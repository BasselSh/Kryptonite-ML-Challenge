{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0539cc1-7130-4dea-a223-0c633f5d804d",
      "metadata": {
        "id": "a0539cc1-7130-4dea-a223-0c633f5d804d"
      },
      "source": [
        "#### Импортируем все необходимые библиотеки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7706ea54-d075-4c1c-bf4f-632e9e32631b",
      "metadata": {
        "id": "7706ea54-d075-4c1c-bf4f-632e9e32631b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "SEED = 2005"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f11340-3afa-44ef-b483-e8b7605cc908",
      "metadata": {
        "id": "45f11340-3afa-44ef-b483-e8b7605cc908"
      },
      "source": [
        "#### Инициализируем генератор случайных чисел."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24325309-92dd-4e2d-b64c-3d1f1559d850",
      "metadata": {
        "id": "24325309-92dd-4e2d-b64c-3d1f1559d850"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bddb5c91-01fe-486d-999b-742dacbcab42",
      "metadata": {
        "id": "bddb5c91-01fe-486d-999b-742dacbcab42"
      },
      "source": [
        "#### Создадим трансформации изображений для тренировочной и валидационной выборок.\n",
        "\n",
        "#### Изображения тренировочной выборки будем поворачивать, брать случайные фрагменты изображения, изменять яркость/контраст/насыщенность.\n",
        "\n",
        "#### Для создания датасета воспользуемся встроенной библиотекой из пакета torchvision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1360fc29-eb91-4488-b15f-9a6646cba85c",
      "metadata": {
        "id": "1360fc29-eb91-4488-b15f-9a6646cba85c",
        "outputId": "cb5a5870-9243-4848-c51a-87790b22d206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset ImageFolder\n",
              "     Number of datapoints: 1997\n",
              "     Root location: train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                RandomRotation(degrees=[-8.0, 8.0], interpolation=nearest, expand=False, fill=0)\n",
              "                RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
              "                RandomHorizontalFlip(p=0.5)\n",
              "                RandomVerticalFlip(p=0.1)\n",
              "                ColorJitter(brightness=(0.9, 1.1), contrast=(0.9, 1.1), saturation=(0.9, 1.1), hue=(-0.03, 0.03))\n",
              "                PILToTensor()\n",
              "                ConvertImageDtype()\n",
              "                Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
              "                RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0, inplace=False)\n",
              "            ),\n",
              " Dataset ImageFolder\n",
              "     Number of datapoints: 1997\n",
              "     Root location: train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
              "                PILToTensor()\n",
              "                ConvertImageDtype()\n",
              "                Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
              "            ))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomRotation(8),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.90, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(p=0.1),\n",
        "    transforms.ColorJitter(brightness=0.10, contrast=0.10, saturation=0.10, hue=0.03),\n",
        "    transforms.PILToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    transforms.RandomErasing(scale=(0.02, 0.10))]\n",
        ")\n",
        "\n",
        "transforms_valid = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.PILToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
        ")\n",
        "\n",
        "dataset_train = datasets.ImageFolder('train', transform = transforms_train)\n",
        "dataset_valid = datasets.ImageFolder('train', transform = transforms_valid)\n",
        "\n",
        "dataset_train, dataset_valid"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9bc597f-f63a-4ec0-8e6c-60c7e6e9e50f",
      "metadata": {
        "id": "e9bc597f-f63a-4ec0-8e6c-60c7e6e9e50f"
      },
      "source": [
        "#### Посмотрим на наши классы изображений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eee206d-70fa-4066-8d77-f80a669b941c",
      "metadata": {
        "id": "0eee206d-70fa-4066-8d77-f80a669b941c",
        "outputId": "01155f17-d2b2-4828-edf6-08af445e6cfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0', '1']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_train.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d69023d-26ee-4746-a4b8-303f5ef8eee2",
      "metadata": {
        "id": "9d69023d-26ee-4746-a4b8-303f5ef8eee2"
      },
      "source": [
        "#### Возьмем предобученную модель VisionTransformer, заменим в ней классификационный слой и инициализируем его."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bbefed-a9fc-48fa-ae1c-a2fd14bf952d",
      "metadata": {
        "id": "a0bbefed-a9fc-48fa-ae1c-a2fd14bf952d",
        "outputId": "094fcd8c-7b99-479d-84f2-92c3c7976b73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16_lc_swag-4e70ced5.pth\" to C:\\Users\\Animados/.cache\\torch\\hub\\checkpoints\\vit_b_16_lc_swag-4e70ced5.pth\n",
            "100%|██████████| 330M/330M [00:30<00:00, 11.2MB/s] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1)\n",
        "model.heads.head = torch.nn.Linear(768, len(dataset_train.classes))\n",
        "\n",
        "torch.nn.init.xavier_uniform_(model.heads.head.weight)\n",
        "torch.nn.init.constant_(model.heads.head.bias, 0.0)\n",
        "model.to('cuda')\n",
        "\n",
        "model.heads"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a1018c2-f929-430c-a32f-6feced4644f1",
      "metadata": {
        "id": "6a1018c2-f929-430c-a32f-6feced4644f1"
      },
      "source": [
        "#### Создадим даталоадеры, которые будут бить данные на батчи и отдавать модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d716eed3-f662-4a6c-8f0b-64871fc4f9aa",
      "metadata": {
        "id": "d716eed3-f662-4a6c-8f0b-64871fc4f9aa"
      },
      "outputs": [],
      "source": [
        "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8,\n",
        "                                           num_workers=4, shuffle=True,\n",
        "                                           drop_last=True, pin_memory=True)\n",
        "loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=8,\n",
        "                                           num_workers=4, shuffle=False,\n",
        "                                           drop_last=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cef83979-0091-44d5-8508-fa50bcaf6633",
      "metadata": {
        "id": "cef83979-0091-44d5-8508-fa50bcaf6633"
      },
      "source": [
        "#### Так как классы не сбалансированы, то для функции потерь посчитаем корректировочные коэффициенты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0eae85-ae8e-4a35-ab8c-5ff13b0fb3a9",
      "metadata": {
        "id": "7c0eae85-ae8e-4a35-ab8c-5ff13b0fb3a9",
        "outputId": "e9b47e76-7fe3-4b1c-f9a1-3560821cc5d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0030], device='cuda:0')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weight = len(dataset_train.targets) / (len(np.unique(dataset_train.targets)) * np.bincount(dataset_train.targets))\n",
        "weight = torch.FloatTensor(weight)\n",
        "weight = torch.nan_to_num(weight, posinf=1.0, neginf=1.0)\n",
        "weight = weight.to('cuda') / weight.min()\n",
        "weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d9abd1-b6ba-4267-ad5f-53298a91b89f",
      "metadata": {
        "id": "54d9abd1-b6ba-4267-ad5f-53298a91b89f"
      },
      "source": [
        "#### Замораживаем предобученные слои и одну эпоху учим только наш классификационный слой. Для ускорения используем тензорные вычисления (AMP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14278368-c3fd-4826-bef6-99f6d0730e46",
      "metadata": {
        "id": "14278368-c3fd-4826-bef6-99f6d0730e46"
      },
      "outputs": [],
      "source": [
        "# pretrain\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.heads.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight)\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad()\n",
        "for imgs, label in loader_train:\n",
        "    pred = model(imgs.to('cuda'))\n",
        "    loss = criterion(pred, label.to('cuda'))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610b08ed-0aac-4604-a42c-7d2a60825b30",
      "metadata": {
        "id": "610b08ed-0aac-4604-a42c-7d2a60825b30"
      },
      "source": [
        "#### Основной цикл вычислений. Размораживаем все слои и учим модель 15 эпох.\n",
        "\n",
        "#### В качестве шедулера возьмем OneCycleLR - он плавно повышает LR до целевого значения и потом сильно понижает его.\n",
        "\n",
        "#### Во время валидации проверяем качество метрики F1. Если метрика не повышается несколько эпох, то загружаем в модель предыдущие лучшие веса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd6ad5d-5e43-4b6d-a5b3-2da6827bb80c",
      "metadata": {
        "id": "fbd6ad5d-5e43-4b6d-a5b3-2da6827bb80c",
        "outputId": "2bb532ca-0c3c-4eca-9dfa-b963b109d7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start epoch 1 at 23:01:58, lr=0.00000050\n",
            "  train loss: 0.3571 valid f1: 0.9072\n",
            "Saved best model!\n",
            "Start epoch 2 at 23:03:15, lr=0.00000135\n",
            "  train loss: 0.1001 valid f1: 0.9820\n",
            "Saved best model!\n",
            "Start epoch 3 at 23:04:32, lr=0.00000325\n",
            "  train loss: 0.0932 valid f1: 0.9770\n",
            "Start epoch 4 at 23:05:49, lr=0.00000478\n",
            "  train loss: 0.1018 valid f1: 0.9910\n",
            "Saved best model!\n",
            "Start epoch 5 at 23:07:07, lr=0.00000497\n",
            "  train loss: 0.0655 valid f1: 0.9744\n",
            "Start epoch 6 at 23:08:24, lr=0.00000475\n",
            "  train loss: 0.0194 valid f1: 0.9518\n",
            "Start epoch 7 at 23:09:40, lr=0.00000434\n",
            "  train loss: 0.0116 valid f1: 0.9754\n",
            "Loading best model weights!\n",
            "Start epoch 8 at 23:10:58, lr=0.00000376\n",
            "  train loss: 0.0599 valid f1: 0.9875\n",
            "Loading best model weights!\n",
            "Start epoch 9 at 23:12:15, lr=0.00000308\n",
            "  train loss: 0.0461 valid f1: 0.9960\n",
            "Saved best model!\n",
            "Start epoch 10 at 23:13:32, lr=0.00000234\n",
            "  train loss: 0.0069 valid f1: 0.9835\n",
            "Start epoch 11 at 23:14:48, lr=0.00000162\n",
            "  train loss: 0.0067 valid f1: 0.9990\n",
            "Saved best model!\n",
            "Start epoch 12 at 23:16:05, lr=0.00000098\n",
            "  train loss: 0.0000 valid f1: 0.9970\n",
            "Start epoch 13 at 23:17:22, lr=0.00000048\n",
            "  train loss: 0.0000 valid f1: 0.9930\n",
            "Start epoch 14 at 23:18:40, lr=0.00000016\n",
            "  train loss: 0.0000 valid f1: 0.9920\n",
            "Loading best model weights!\n",
            "Start epoch 15 at 23:19:57, lr=0.00000005\n",
            "  train loss: 0.0000 valid f1: 0.9975\n",
            "Loading best model weights!\n"
          ]
        }
      ],
      "source": [
        "epochs = 15\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, epochs=epochs, max_lr=5e-6,\n",
        "                                                   div_factor=10.0, final_div_factor=10.0,\n",
        "                                                   steps_per_epoch=1)\n",
        "\n",
        "best_f1  = 0\n",
        "best_cnt = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Start epoch {epoch + 1} at {datetime.now().strftime('%H:%M:%S')}, lr={current_lr:0.8f}\")\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_losses = []\n",
        "    for imgs, label in loader_train:\n",
        "        pred = model(imgs.to('cuda'))\n",
        "        loss = criterion(pred, label.to('cuda'))\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    val_label = []\n",
        "    trg_label = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, label in loader_valid:\n",
        "            pred = model(imgs.to('cuda'))\n",
        "            trg_label.extend(label.numpy().tolist())\n",
        "            val_label.extend(pred.argmax(dim=1).cpu().numpy().flatten().tolist())\n",
        "    f1_label = f1_score(trg_label, val_label, zero_division=0, average='macro')\n",
        "\n",
        "    print(f\"  train loss: {np.mean(train_losses):6.4f} valid f1: {f1_label:6.4f}\")\n",
        "\n",
        "    if f1_label > best_f1:\n",
        "        best_f1 = f1_label\n",
        "        best_cnt = 0\n",
        "        torch.save(model.state_dict(), \"model.pth\")\n",
        "        print(\"Saved best model!\")\n",
        "    else:\n",
        "        best_cnt += 1\n",
        "\n",
        "    if best_cnt > 2:\n",
        "        print(\"Loading best model weights!\")\n",
        "        model.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1438df9-4ca2-4af1-821b-c00e9986832c",
      "metadata": {
        "scrolled": true,
        "id": "b1438df9-4ca2-4af1-821b-c00e9986832c",
        "outputId": "8d6b984f-9590-435b-d2f7-627e3efe703d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '0', 1: '1'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rev_idx = {v:k for k, v in dataset_train.class_to_idx.items()}\n",
        "rev_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b93f86b-200f-43e1-924f-850c8236fb49",
      "metadata": {
        "id": "9b93f86b-200f-43e1-924f-850c8236fb49"
      },
      "source": [
        "# Предсказание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7731c504-484f-4f0e-b115-4d673b086a5f",
      "metadata": {
        "id": "7731c504-484f-4f0e-b115-4d673b086a5f",
        "outputId": "0b4c9710-2bf9-4205-a4e8-72edd6e556f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu126\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{0: '0', 1: '1'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "\n",
        "DEVICE = 'cuda' #'cpu'\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "rev_idx = {0: '0', 1: '1'}\n",
        "rev_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac3ed0c9-f7c0-4ad2-a1f4-9b17574c93d5",
      "metadata": {
        "id": "ac3ed0c9-f7c0-4ad2-a1f4-9b17574c93d5"
      },
      "source": [
        "#### Подгружаем модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554a634d-84cf-4549-98e4-3c6df6af5690",
      "metadata": {
        "id": "554a634d-84cf-4549-98e4-3c6df6af5690",
        "outputId": "5e46ed2e-b4fb-401f-e054-3dd4c62a092e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.PILToTensor(),\n",
        "    transforms.ConvertImageDtype(torch.float),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
        ")\n",
        "\n",
        "model = models.vit_b_16(weights=None)\n",
        "model.heads.head = torch.nn.Linear(768, len(rev_idx))\n",
        "\n",
        "model.to(DEVICE)\n",
        "state_dict = torch.load(\"model.pth\", map_location=DEVICE)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "model.heads"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4fa279-2925-4442-b567-0693d2e8929d",
      "metadata": {
        "id": "1f4fa279-2925-4442-b567-0693d2e8929d"
      },
      "source": [
        "#### Считываем тестовые картинки, сортируем список и подаем по одной в нашу модель.\n",
        "\n",
        "#### Функция argmax даем нам наиболее вероятный класс изображения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849654fd-7195-4b7e-b519-a7b0f88d6de1",
      "metadata": {
        "id": "849654fd-7195-4b7e-b519-a7b0f88d6de1",
        "outputId": "1715a206-0a9a-4b1f-e6a6-268f5c625276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 6h 54min 3s\n",
            "Wall time: 52min\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('00000000.jpg', '1'),\n",
              " ('00000001.jpg', '1'),\n",
              " ('00000002.jpg', '1'),\n",
              " ('00000003.jpg', '1'),\n",
              " ('00000004.jpg', '0')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "dirname = r'test'\n",
        "allpy = glob.glob(dirname + os.sep + '*')\n",
        "res = []\n",
        "for filename in allpy:\n",
        "    with Image.open(filename).convert('RGB') as img:\n",
        "        img.load()\n",
        "        with torch.no_grad():\n",
        "            pred = model(transforms_test(img).unsqueeze(0).cuda())\n",
        "        res.append((filename.split(\"\\\\\")[-1], rev_idx[pred.argmax().item()]))\n",
        "\n",
        "res[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2232ba90-0f75-40b8-9e57-7217183f2ef3",
      "metadata": {
        "id": "2232ba90-0f75-40b8-9e57-7217183f2ef3"
      },
      "outputs": [],
      "source": [
        "# Новый список без расширения .jpg\n",
        "stripped_list = [(filename.replace('.jpg', ''), label) for filename, label in res]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d494033a-b98a-4638-addc-1c32f1c867eb",
      "metadata": {
        "id": "d494033a-b98a-4638-addc-1c32f1c867eb",
        "outputId": "88bf2af9-6d25-4874-910e-ce8b6f1dfe2e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_id</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000001</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00000002</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00000003</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00000004</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161186</th>\n",
              "      <td>00161186</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161187</th>\n",
              "      <td>00161187</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161188</th>\n",
              "      <td>00161188</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161189</th>\n",
              "      <td>00161189</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161190</th>\n",
              "      <td>00161190</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>161191 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         pair_id similarity\n",
              "0       00000000          1\n",
              "1       00000001          1\n",
              "2       00000002          1\n",
              "3       00000003          1\n",
              "4       00000004          0\n",
              "...          ...        ...\n",
              "161186  00161186          1\n",
              "161187  00161187          1\n",
              "161188  00161188          1\n",
              "161189  00161189          1\n",
              "161190  00161190          1\n",
              "\n",
              "[161191 rows x 2 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(stripped_list, columns=['pair_id', 'similarity'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a549fb0d-748f-46bd-8d0f-16a6e08f11e0",
      "metadata": {
        "id": "a549fb0d-748f-46bd-8d0f-16a6e08f11e0"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"submission_VIT.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7aa9226-5856-4aef-85f5-bbb4bad76e4b",
      "metadata": {
        "id": "d7aa9226-5856-4aef-85f5-bbb4bad76e4b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}